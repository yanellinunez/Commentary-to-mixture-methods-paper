---
title: "Subset_lasso"
author: "Yanelli Nunez"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
editor_options: 
  chunk_output_type: console
---

-If I give you seed 3 I get a full range and if I give you seed 1 I get a single point. 
-Do group lasso with single element groupings and see if we can replicate lasso
-what is the vector matrix structure that we suppose to use for grp lasso 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(janitor)
library(caret)
library(Hmisc)
library(glmnet)
library(grpreg)
```

Note: R is version: 3.6.1
glmnet and grpreg package are the new versions
```{r}
sessionInfo()
```

### Data Import and Cleaning 

First, load the dataset; clean up names as needed; and convert factors to, well, factors. Next we remove missing values and reorder predictors (environmental variables first, confounders second). In keeping with standard practice, we'll ln-transform the environmental exposures and the outcome. This is the dataset we'll use to illustrate variable selection methods.

```{r}
study_pop = read_csv("Data/studypop.csv") %>% 
  clean_names(case = c("old_janitor")) %>% 
  mutate(bmi_cat3 = as.factor(bmi_cat3),
         edu_cat = as.factor(edu_cat),
         race_cat = as.factor(race_cat),
         male = as.factor(male)) 

data = study_pop %>% 
  mutate_at(vars(contains("la")), log) %>% 
  mutate(log_telomean = log(telomean)) %>% 
  dplyr::select(log_telomean, lbx074la:lbx187la, lbxd03la:lbx194la, everything(), -seqn, -telomean) %>% 
  na.omit(log_telomean) 

subset = data[1:100,]

# Create a matrix of predictors as x
x = model.matrix(log_telomean ~ ., subset)[,-1]

# Extract outcome vector
y = subset$log_telomean
```

### Lasso w/ CV

Some built-in functions will conduct a cross-validation analysis and identify the "best" tuning parameter.
Optional arguments to `glmnet` can be useful -- in particular, `weights` can be used in the context of the adaptive lasso and `penalty.factor` can separate penalized variables from confounders. 

```{r}
set.seed(2)

is_penalized = c(rep(1, ncol(x[,1:18])), rep(0, ncol(x[,19:36])))

# Use cross-validation (CV) to find best lambda value
cv.lasso = cv.glmnet(x, y, 
                     penalty.factor = is_penalized,
                     type.measure = "mse", alpha = 1)
plot(cv.lasso)

best_lambda = cv.lasso$lambda.min
best_lambda
```


### Looping seeds for lasso for coefficients plot
```{r}
fit_cv_lasso = function(seed) {
  set.seed(seed)
  cv.lasso = cv.glmnet(x, y, 
                     penalty.factor = c(rep(1, ncol(x[,1:18])), rep(0, ncol(x[,19:36]))),
                     type.measure = "mse", alpha = 1)
  
  #Find the lambda that results in the smallest CV error
  best_lambda <- cv.lasso$lambda.min
 
  
  #Fit model with cross-validated lambda
  lasso.mod = glmnet(x, y, 
                   penalty.factor = c(rep(1, ncol(x[,1:18])), rep(0, ncol(x[,19:36]))),
                   alpha = 1, lambda = best_lambda)

lasso.mod
}


extract_coefs = function(fit) {
 as.tibble(as.matrix(fit$beta), rownames = "variable") %>%
    rename(beta = s0) 
}


repeat_cv_lasso = 
  tibble(seed = 1:100) %>% 
  mutate(
    fits = map(seed, ~fit_cv_lasso(seed = .x)),
    coefs = map(fits, extract_coefs)) %>% 
  dplyr::select(-fits) %>% 
  unnest() %>%
  rename(beta_lasso = "beta") 


```

### Looping seeds for lasso for lambda plot
```{r}

fit_cv_lasso_2 = function(seed) {
  set.seed(seed)
  cv.lasso = cv.glmnet(x, y, 
                     penalty.factor = c(rep(1, ncol(x[,1:18])), rep(0, ncol(x[,19:36]))),
                     type.measure = "mse", alpha = 1)

  
 loop_cvm    <- cv.lasso$cvm  # mean cross-validation error
  loop_cvsd   <- cv.lasso$cvsd #std error of CV error
  loop_lam    <- cv.lasso$lambda 
  
  dat <- cbind(lam = loop_lam, cvm = loop_cvm, cvsd = loop_cvsd) %>% 
    as_tibble() %>% 
    mutate(upper = cvm + 1.96*cvsd,
         lower = cvm - 1.96*cvsd) %>% 
    mutate(seed1 = seed)

}

repeat_cv_lasso_2 = 
  tibble(seed = 1:100) %>%
  mutate(
    fits = map(seed, ~fit_cv_lasso_2(seed = .x))) %>% 
  unnest() %>% 
  mutate(seed = as.factor(seed))

  
```


# Lasso visualization

#### betas
```{r}
repeat_cv_lasso  %>%
  ggplot() + 
  geom_point(aes(x = seed, y = beta_lasso), color = 'red', size = 0.5, alpha = 0.5) + 
  facet_wrap(~variable) + 
  geom_hline(yintercept = 0) +
  ggtitle("100 observations")

```


### Lambda with Error Bars
```{r}
repeat_cv_lasso_2 %>% 
  ggplot(aes(x = lam, y = cvm, color = seed)) + 
  geom_point() + 
  geom_line() +
  geom_errorbar(aes(ymin = lower, ymax = upper)) +
  theme(legend.position = "none") +
  labs(x = expression(lambda), y = "Cross-Validation Error")
```


### Lambda without Error Bars
```{r}
repeat_cv_lasso_2 %>% 
  ggplot(aes(x = lam, y = cvm, color = seed)) + 
  geom_point() + 
  geom_line() +
  theme(legend.position = "none") +
  labs(x = expression(lambda), y = "Cross-Validation Error")
```


### Selected lambda
```{r}
min_cv <- repeat_cv_lasso_2 %>% 
  group_by(seed) %>% 
  filter(cvm == min(cvm))

min_cv %>% 
  ggplot(aes(y = lam, x = seed)) + 
  geom_point(aes(color = lam)) + 
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 90, size = 7)) +
  labs(x = "seed", y = expression("Cross-Validation Selected "*lambda))
```

