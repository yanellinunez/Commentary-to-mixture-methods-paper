---
title: "Elastic_net"
author: "Yanelli Nunez"
date: "10/21/2019"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
editor_options: 
  chunk_output_type: console
---

-If I give you seed 3 I get a full range and if I give you seed 1 I get a single point. 
-Do group lasso with single element groupings and see if we can replicate lasso
-what is the vector matrix structure that we suppose to use for grp lasso 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(janitor)
library(caret)
library(Hmisc)
library(glmnet)
library(grpreg)
```

# Data Import and Cleaning 

First, load the dataset; clean up names as needed; and convert factors to, well, factors. Next we remove missing values and reorder predictors (environmental variables first, confounders second). In keeping with standard practice, we'll ln-transform the environmental exposures and the outcome. This is the dataset we'll use to illustrate variable selection methods.

```{r}
study_pop = read_csv("Data/studypop.csv") %>% 
  clean_names(case = c("old_janitor")) %>% 
  mutate(bmi_cat3 = as.factor(bmi_cat3),
         edu_cat = as.factor(edu_cat),
         race_cat = as.factor(race_cat),
         male = as.factor(male)) 

data = study_pop %>% 
  mutate_at(vars(contains("la")), log) %>% 
  mutate(log_telomean = log(telomean)) %>% 
  dplyr::select(log_telomean, lbx074la:lbx187la, lbxd03la:lbx194la, everything(), -seqn, -telomean) %>% 
  na.omit(log_telomean) 

# Create a matrix of predictors as x
x = model.matrix(log_telomean ~ ., data)[,-1]

# Extract outcome vector
y = data$log_telomean
```


# Elastic Net
### Elastic Net w/ CV

`glmnet` implements the elastic net -- we've set `alpha` to 1 to focus on lasso regression, but could choose `alpha` between 0 and 1. In the following, we can tune alpha and lambda simultaneously.

```{r enalpha, results = "hide"}
# For reproducibility, set a seed
set.seed(2)

# Use CV to select best alpha value
## Create a tuning grid of alpha and lambda values
egrid <- expand.grid(alpha = (1:10) * 0.1, 
                     lambda = .5^(3:20))

## Create a tuning control for cv
control <- trainControl(method = "repeatedcv", 
                        repeats = 3, 
                        verboseIter = TRUE)

## Use the tuning grid and control to find best alpha
elnetFit <- train(x = x, 
            y = y,
            method = "glmnet",
            penalty.factor = c(rep(1, ncol(x[,1:18])), rep(0, ncol(x[,19:36]))),
            tuneGrid = egrid,
            trControl = control)
plot(elnetFit)

best_alpha = elnetFit$bestTune$alpha
```

For the optimal `alpha` value, we identify a best lambda and explore the results. 

```{r elnet}
set.seed(2)

# Use CV to find best lambda value
cv.elnet = cv.glmnet(x, y, 
                     penalty.factor = c(rep(1, ncol(x[,1:18])), rep(0, ncol(x[,19:36]))),
                     type.measure = "mse", alpha = best_alpha)
plot(cv.elnet)

best_elambda = cv.elnet$lambda.min
best_elambda

elnet.mod = glmnet(x, y, 
                   penalty.factor = c(rep(1, ncol(x[,1:18])), rep(0, ncol(x[,19:36]))),
                   alpha = best_alpha, lambda = best_elambda)

coef_elnet = coef(elnet.mod)
coef_elnet

```


### Looping seeds for elastic net coefficients plot
```{r}
fit_cv_elnet = function(seed) {
  set.seed(seed)
  cv.elnet = cv.glmnet(x, y, 
                     penalty.factor = c(rep(1, ncol(x[,1:18])), rep(0, ncol(x[,19:36]))),
                     type.measure = "mse", alpha = 0.1)
  
  #Find the lambda that results in the smallest CV error
  best_lambda = cv.elnet$lambda.min
  
  #Fit model with cross-validated lambda
  elnet.mod = glmnet(x, y, 
                   penalty.factor = c(rep(1, ncol(x[,1:18])), rep(0, ncol(x[,19:36]))),
                   alpha = 0.1, lambda = best_lambda)
  
elnet.mod
}
extract_coefs = function(fit) {
 as.tibble(as.matrix(fit$beta), rownames = "variable") %>%
    rename(beta = s0) 
} 

repeat_cv_elnet = 
  tibble(seed = 1:100) %>% 
  mutate(
    fits = map(seed, ~fit_cv_elnet(seed = .x)),
    coefs = map(fits, extract_coefs)) %>% 
  dplyr::select(-fits) %>% 
  unnest() %>%
  rename(beta_elnet = "beta")
```


### Looping seeds for elastic net lambda plot
```{r}

fit_cv_elnet_2 = function(seed) {
  set.seed(seed)
  cv.lasso = cv.glmnet(x, y, 
                     penalty.factor = c(rep(1, ncol(x[,1:18])), rep(0, ncol(x[,19:36]))),
                     type.measure = "mse", alpha = 0.1)

  
 loop_cvm    <- cv.lasso$cvm  # mean cross-validation error
  loop_cvsd   <- cv.lasso$cvsd #std error of CV error
  loop_lam    <- cv.lasso$lambda 
  
  dat <- cbind(lam = loop_lam, cvm = loop_cvm, cvsd = loop_cvsd) %>% 
    as_tibble() %>% 
    mutate(upper = cvm + 1.96*cvsd,
         lower = cvm - 1.96*cvsd) %>% 
    mutate(seed1 = seed)

}

repeat_cv_elnet_2 = 
  tibble(seed = 1:100) %>%
  mutate(
    fits = map(seed, ~fit_cv_elnet_2(seed = .x))) %>% 
  unnest() %>% 
  mutate(seed = as.factor(seed))

  
```


# Data Visualization

### Betas
```{r}
repeat_cv_elnet %>%
  ggplot() + 
  geom_point(aes(x = seed, y = beta_elnet), color = 'blue', size = 0.5, alpha = 0.4) + 
  facet_wrap(~variable) +
  geom_hline(yintercept = 0, color = "red")

```


### Lambda With Error Bars
```{r}
repeat_cv_elnet_2 %>% 
  ggplot(aes(x = lam, y = cvm, color = seed)) + 
  geom_point() + 
  geom_line() +
  geom_errorbar(aes(ymin = lower, ymax = upper)) +
  theme(legend.position = "none") +
  labs(x = expression(lambda), y = "Cross-Validation Error")
```


### Lambda without Error Bars
```{r}
repeat_cv_elnet_2 %>% 
  ggplot(aes(x = lam, y = cvm, color = seed)) + 
  geom_point() + 
  geom_line() +
  theme(legend.position = "none") +
  labs(x = expression(lambda), y = "Cross-Validation Error")
```


### Selected lambda
```{r}
min_cv <- repeat_cv_elnet_2 %>% 
  group_by(seed) %>% 
  filter(cvm == min(cvm))

min_cv %>% 
  ggplot(aes(y = lam, x = seed)) + 
  geom_point(aes(color = lam)) + 
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 90, size = 7)) +
  labs(x = "seed", y = expression("Cross-Validation Selected "*lambda))
```
