---
title: "beta coefficients plot"
author: "Yanelli Nunez"
date: "7/12/2019"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(janitor)
library(caret)
library(Hmisc)
library(glmnet)
library(grpreg)
```

# Data Import and Cleaning 

First, load the dataset; clean up names as needed; and convert factors to, well, factors. Next we remove missing values and reorder predictors (environmental variables first, confounders second). In keeping with standard practice, we'll ln-transform the environmental exposures and the outcome. This is the dataset we'll use to illustrate variable selection methods.

```{r}
study_pop = read_csv("Data/studypop.csv") %>% 
  clean_names(case = c("old_janitor")) %>% 
  mutate(bmi_cat3 = as.factor(bmi_cat3),
         edu_cat = as.factor(edu_cat),
         race_cat = as.factor(race_cat),
         male = as.factor(male)) 

data = study_pop %>% 
  mutate_at(vars(contains("la")), log) %>% 
  mutate(log_telomean = log(telomean)) %>% 
  dplyr::select(log_telomean, lbx074la:lbx187la, lbxd03la:lbx194la, everything(), -seqn, -telomean) %>% 
  na.omit(log_telomean) 

# Create a matrix of predictors as x
x = model.matrix(log_telomean ~ ., data)[,-1]

# Extract outcome vector
y = data$log_telomean
```

# Lasso w/ CV

Some built-in functions will conduct a cross-validation analysis and identify the "best" tuning parameter.
Optional arguments to `glmnet` can be useful -- in particular, `weights` can be used in the context of the adaptive lasso and `penalty.factor` can separate penalized variables from confounders. 

```{r}
set.seed(2)

is_penalized = c(rep(1, ncol(x[,1:18])), rep(0, ncol(x[,19:36])))

# Use cross-validation (CV) to find best lambda value
cv.lasso = cv.glmnet(x, y, 
                     penalty.factor = is_penalized,
                     type.measure = "mse", alpha = 1)
plot(cv.lasso)

best_lambda = cv.lasso$lambda.min
best_lambda
```

Let's examine the model with the best CV score. 

```{r}
# Lasso model using cross-validated lambda value
lasso.mod = glmnet(x, y, 
                   penalty.factor = is_penalized,
                   alpha = 1, lambda = best_lambda)

coef_lasso = coef(lasso.mod)
lasso.mod$beta

# Find the number of non-zero estimates
length(coef_lasso[coef_lasso != 0]) # 23 non-zero estimates including the intercept

# Find variables that are non-zero
dimnames(coef_lasso)[[1]][which(coef_lasso != 0)]

# Find the MSE
lasso.pred <-  predict(lasso.mod, newx = x)
lasso_mse <- mean((lasso.pred - y)^2)
lasso_mse

#The final code chunk saves the best lasso coefficients for later use. 
lasso_beta = cbind(rownames(coef_lasso), as.vector(coef_lasso)) %>% 
  as.tibble() %>% 
  rename(variable = 1, beta = 2) %>% 
  mutate(beta = as.numeric(beta)) %>% 
  filter(variable != "(Intercept)") %>% 
  mutate(method = "Lasso")

```


#functions for looping seeds for lasso
```{r}
fit_cv_lasso = function(seed) {
  set.seed(seed)
  cv.lasso = cv.glmnet(x, y, 
                     penalty.factor = c(rep(1, ncol(x[,1:18])), rep(0, ncol(x[,19:36]))),
                     type.measure = "mse", alpha = 1)
  
  #Find the lambda that results in the smallest CV error
  best_lambda = cv.lasso$lambda.min
  
  #Fit model with cross-validated lambda
  lasso.mod = glmnet(x, y, 
                   penalty.factor = c(rep(1, ncol(x[,1:18])), rep(0, ncol(x[,19:36]))),
                   alpha = 1, lambda = best_lambda)
  
lasso.mod
}


extract_coefs = function(fit) {
 as.tibble(as.matrix(fit$beta), rownames = "variable") %>%
    rename(beta = s0) 
}

```


## lasso beta values for different seeds
```{r}
repeat_cv_lasso = 
  tibble(seed = 1:100) %>% 
  mutate(
    fits = map(seed, ~fit_cv_lasso(seed = .x)),
    coefs = map(fits, extract_coefs)) %>% 
  dplyr::select(-fits) %>% 
  unnest() %>%
  rename(beta_lasso = "beta") 
  
```


# Elastic Net w/ CV

`glmnet` implements the elastic net -- we've set `alpha` to 1 to focus on lasso regression, but could choose `alpha` between 0 and 1. In the following, we can tune alpha and lambda simultaneously.

```{r enalpha, results = "hide"}
# For reproducibility, set a seed
set.seed(2)

# Use CV to select best alpha value
## Create a tuning grid of alpha and lambda values
egrid <- expand.grid(alpha = (1:10) * 0.1, 
                     lambda = .5^(3:20))

## Create a tuning control for cv
control <- trainControl(method = "repeatedcv", 
                        repeats = 3, 
                        verboseIter = TRUE)

## Use the tuning grid and control to find best alpha
elnetFit <- train(x = x, 
            y = y,
            method = "glmnet",
            penalty.factor = is_penalized,
            tuneGrid = egrid,
            trControl = control)
plot(elnetFit)

best_alpha = elnetFit$bestTune$alpha
```

For the optimal `alpha` value, we identify a best lambda and explore the results. 

```{r elnet}
set.seed(2)

# Use CV to find best lambda value
cv.elnet = cv.glmnet(x, y, 
                     penalty.factor = is_penalized,
                     type.measure = "mse", alpha = best_alpha)
plot(cv.elnet)

best_elambda = cv.elnet$lambda.min
best_elambda

elnet.mod = glmnet(x, y, 
                   penalty.factor = is_penalized,
                   alpha = best_alpha, lambda = best_elambda)

coef_elnet = coef(elnet.mod)
coef_elnet

# Find the number of non-zero estimates
length(coef_elnet[coef_elnet != 0]) # 24 non-zero estimates including the intercept

# Find variables that are non-zero
dimnames(coef_elnet)[[1]][which(coef_elnet != 0)]

# Find the MSE
elnet.pred <-  predict(elnet.mod, newx = x)
elnet_mse <- mean((elnet.pred - y)^2)
elnet_mse
```


#function for looping seeds for elastic net
```{r}
fit_cv_elnet = function(seed) {
  set.seed(seed)
  cv.elnet = cv.glmnet(x, y, 
                     penalty.factor = c(rep(1, ncol(x[,1:18])), rep(0, ncol(x[,19:36]))),
                     type.measure = "mse", alpha = 0.1)
  
  #Find the lambda that results in the smallest CV error
  best_lambda = cv.elnet$lambda.min
  
  #Fit model with cross-validated lambda
  elnet.mod = glmnet(x, y, 
                   penalty.factor = c(rep(1, ncol(x[,1:18])), rep(0, ncol(x[,19:36]))),
                   alpha = 0.1, lambda = best_lambda)
  
elnet.mod
}

```

# Elastic net beta values for different seeds
```{r}
repeat_cv_elnet = 
  tibble(seed = 1:100) %>% 
  mutate(
    fits = map(seed, ~fit_cv_elnet(seed = .x)),
    coefs = map(fits, extract_coefs)) %>% 
  dplyr::select(-fits) %>% 
  unnest() %>%
  rename(beta_elnet = "beta")
```

#Group lasso

Variable grouping for group lasso

```{r}
group3 <- vector()
group3[grepl("lbx1|0",colnames(x))] <- "Non-Dioxin-like PCB"
group3[grepl("lbxd", colnames(x))] <- "TEQ"
group3[grepl("lbxf", colnames(x))] <- "TEQ"
group3[grepl("lbxh|p", colnames(x))] <- "Non-Ortho PCB"
group3[grepl("lbx118la", colnames(x))] <- "TEQ"
group3[grepl("pct", colnames(x))] <- "0"
group3[grepl("bcsi", colnames(x))] <- "0"
group3[grepl("bmi", colnames(x))] <- "0"
group3[grepl("edu", colnames(x))] <- "0"
group3[grepl("race", colnames(x))] <- "0"
group3[grepl("male", colnames(x))] <- "0"
group3[grepl("bxcot", colnames(x))] <- "0"
group3[grepl("age", colnames(x))] <- "0"
group3 <- as.factor(group3)

cbind(colnames(x), group3) #bind by columns
```

This doesn't work when we have the right grouping 
```{r}
cv_lasso_3 <- cv.grpreg(x, y, group3, 
                        penalty = "grLasso", seed = 1988)

plot(cv_lasso_3)
```


Functions
```{r}
fit_with_cv = function(seed) {
  cv_lasso_3 <- cv.grpreg(x, y, group3, 
                          penalty = "grLasso", seed = seed)
  
  #Find the lambda that results in the smallest CV error
  best_lasso_3 <- cv_lasso_3$lambda.min
  
  #Fit model with cross-validated lambda
  fit_lasso_3 <- grpreg(x, y, group3, penalty = "grLasso", lambda = best_lasso_3)
  
  fit_lasso_3
}

extract_coefs_grlasso = function(fit) {
  as_tibble(fit$beta, rownames = "variable") %>% 
    rename(beta = 2) %>% 
    filter(variable != "(Intercept)")  %>% 
    mutate(group3 = group3) %>% 
    filter(group3 %in% c("TEQ", "Non-Dioxin-like PCB", "Non-Ortho PCB"))
}
```

Looping for different seeds for group lasso
```{r}
repeat_cv_group_lasso = 
  tibble(seed = 1:100) %>% 
  mutate(
    fits = map(seed, ~fit_with_cv(seed = .x)),
    coefs = map(fits, extract_coefs_grlasso)) %>% 
  dplyr::select(-fits) %>% 
  unnest() %>%
  rename(beta_grlasso = "beta")
  

```


#combined data sets for plotting
```{r}
betas <- full_join(repeat_cv_elnet, repeat_cv_lasso, by = c("variable", "seed")) %>%
  full_join(., repeat_cv_group_lasso, by = c("variable", "seed")) 
          
```
                            
#Combined plot 

```{r}
betas_plot <- betas %>%
  ggplot() + 
  geom_point(aes(x = seed, y = beta_lasso, color = 'red'), size = 0.5, alpha = 0.7) + 
  geom_point(aes(x = seed, y = beta_elnet, color = 'blue'), size = 0.5, alpha = 0.4) +
  geom_point(aes(x = seed, y = beta_grlasso, color = 'dark green'), size = 0.5, alpha = 0.5) +
  facet_wrap(~variable) +
  ggtitle("Beta values for different seeds") +
  labs(y = "beta") +
  scale_colour_manual(name = "", 
        values =c('blue'='blue','red'='red', 'dark green' = 'dark green'), labels = c('Elastic Net','Lasso', 'group Lasso'))
betas_plot
```


#Individual plots
```{r}
betas_lasso <- betas %>%
  ggplot() + 
  geom_point(aes(x = seed, y = beta_lasso), color = 'red', size = 0.5, alpha = 0.7) + 
  facet_wrap(~variable)
betas_lasso

betas_elnet <- betas %>%
  ggplot() + 
  geom_point(aes(x = seed, y = beta_elnet), color = 'blue', size = 0.5, alpha = 0.4) + 
  facet_wrap(~variable)
betas_elnet

#inconsistend coefficients when looping the seed
betas_grplasso <- betas %>%
  ggplot() + 
  geom_point(aes(x = seed, y = beta_grlasso), color = 'dark green', size = 0.5, alpha = 0.5) + 
  facet_wrap(~variable)
betas_grplasso
```

Note: R is version: 3.6.1
glmnet and grpreg package are the older versions
```{r}
sessionInfo()
```

